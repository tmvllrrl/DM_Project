{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov  5 08:24:46 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  RTX A6000           On   | 00000000:05:00.0 Off |                  Off |\n",
      "| 30%   40C    P0    34W / 300W |      1MiB / 48685MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import random \n",
    "import glob\n",
    "#from google.colab import drive\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re \n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn.modules.linear import Linear \n",
    "from torch.optim.optimizer import Optimizer, required \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import torchvision.models as models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Utils\n",
    "to_tensor_trans = transforms.ToTensor()\n",
    "def get_img_paths(src_dir):\n",
    "    return [os.path.join(src_dir, img_name) for img_name in os.listdir(src_dir)]\n",
    "\n",
    "class DataGenerator(Dataset):\n",
    "    def __init__(self, phase, img_dir, s=0.5):\n",
    "        self.phase = phase\n",
    "        self.img_paths = get_img_paths(img_dir)\n",
    "        #print(self.img_paths[0:10])\n",
    "        self.s = s\n",
    "        self.transforms = transforms.Compose([\n",
    "                                                transforms.RandomHorizontalFlip(0.5),\n",
    "                                                transforms.RandomResizedCrop((66, 200), (0.8, 1.0)),\n",
    "                                                transforms.Compose([transforms.RandomApply([transforms.ColorJitter(0.8*self.s, \n",
    "                                                                                                                    0.8*self.s, \n",
    "                                                                                                                    0.8*self.s, \n",
    "                                                                                                                    0.2*self.s)], p=0.8),\n",
    "                                                                    transforms.RandomGrayscale(p=0.2)]),\n",
    "                                                transforms.GaussianBlur(7),\n",
    "                                            ])\n",
    "\n",
    "\n",
    "        # These are imagenet channel wise means and stds, change them to fit Honda dataset?\n",
    "        self.mean = np.array([[[[0.485]], [[0.456]], [[0.406]]]])\n",
    "        self.std = np.array([[[[0.229]], [[0.224]], [[0.225]]]])\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.img_paths)\n",
    "    \n",
    "    def _get_img(self, idx):\n",
    "        path = self.img_paths[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        # Putting this hear instead of in self.transforms so it will work for both train/val\n",
    "        x = to_tensor_trans(img)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self._get_img(idx)\n",
    "        x1 = self.augment(x)\n",
    "        x2 = self.augment(x)\n",
    "\n",
    "        x1 = self.preprocess(x1)\n",
    "        x2 = self.preprocess(x2)\n",
    "\n",
    "        return x1, x2\n",
    "    \n",
    "    def preprocess(self, frame):\n",
    "        frame = (frame - self.mean) / self.std\n",
    "        return frame\n",
    "    \n",
    "    def augment(self, frame, transformations=None):\n",
    "        if self.phase == 'train':\n",
    "            return self.transforms(frame)\n",
    "        else:\n",
    "            return frame # Dont we need these transformations to be applied at validation ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "\n",
    "EETA_DEFAULT = 0.001\n",
    "\n",
    "class LARS(Optimizer):\n",
    "    \"\"\"\n",
    "    Layer wise Adaptive Rate Scaling\n",
    "    exclude_from_weight_decay = \n",
    "    exclude_from_layer_adaptation =\n",
    "    classic_momentum = whether or not to use the regular momentum \n",
    "    \"\"\"\n",
    "    def __init__(self, params, \n",
    "    lr = required, momentum= 0.9, \n",
    "    use_nesterov= False, \n",
    "    weight_decay= 0.0, \n",
    "    exclude_from_weight_decay = None, \n",
    "    exclude_from_layer_adaptation = None, \n",
    "    classic_momentum=True,\n",
    "    eeta = EETA_DEFAULT):\n",
    "\n",
    "        self.epoch = 0\n",
    "        defaults = dict(\n",
    "            lr=lr,\n",
    "            momentum=momentum,\n",
    "            use_nesterov=use_nesterov,\n",
    "            weight_decay=weight_decay,\n",
    "            exclude_from_weight_decay=exclude_from_weight_decay,\n",
    "            exclude_from_layer_adaptation=exclude_from_layer_adaptation,\n",
    "            classic_momentum=classic_momentum,\n",
    "            eeta=eeta,\n",
    "        )\n",
    "\n",
    "        super(LARS, self).__init__(params, defaults)\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.weight_decay = weight_decay\n",
    "        self.use_nesterov = use_nesterov\n",
    "        self.classic_momentum = classic_momentum\n",
    "        self.eeta = eeta\n",
    "        self.exclude_from_weight_decay = exclude_from_weight_decay\n",
    "        # exclude_from_layer_adaptation is set to exclude_from_weight_decay if the\n",
    "        # arg is None.\n",
    "        if exclude_from_layer_adaptation:\n",
    "            self.exclude_from_layer_adaptation = exclude_from_layer_adaptation\n",
    "        else:\n",
    "            self.exclude_from_layer_adaptation = exclude_from_weight_decay\n",
    "\n",
    "    def step(self, epoch=None, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        if epoch is None:\n",
    "            epoch = self.epoch\n",
    "            self.epoch += 1\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group[\"weight_decay\"]\n",
    "            momentum = group[\"momentum\"]\n",
    "            eeta = group[\"eeta\"]\n",
    "            lr = group[\"lr\"]\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "\n",
    "                param = p.data\n",
    "                grad = p.grad.data\n",
    "\n",
    "                param_state = self.state[p]\n",
    "\n",
    "                # TODO: get param names\n",
    "                # if self._use_weight_decay(param_name):\n",
    "                grad += self.weight_decay * param\n",
    "\n",
    "                if self.classic_momentum:\n",
    "                    trust_ratio = 1.0\n",
    "\n",
    "                    # TODO: get param names\n",
    "                    # if self._do_layer_adaptation(param_name):\n",
    "                    w_norm = torch.norm(param)\n",
    "                    g_norm = torch.norm(grad)\n",
    "\n",
    "                    device = g_norm.get_device()\n",
    "                    trust_ratio = torch.where(\n",
    "                        w_norm.ge(0),\n",
    "                        torch.where(\n",
    "                            g_norm.ge(0),\n",
    "                            (self.eeta * w_norm / g_norm),\n",
    "                            torch.Tensor([1.0]).to(device),\n",
    "                        ),\n",
    "                        torch.Tensor([1.0]).to(device),\n",
    "                    ).item()\n",
    "\n",
    "                    scaled_lr = lr * trust_ratio\n",
    "                    if \"momentum_buffer\" not in param_state:\n",
    "                        next_v = param_state[\"momentum_buffer\"] = torch.zeros_like(\n",
    "                            p.data\n",
    "                        )\n",
    "                    else:\n",
    "                        next_v = param_state[\"momentum_buffer\"]\n",
    "\n",
    "                    next_v.mul_(momentum).add_(other=grad, alpha=scaled_lr)\n",
    "                    if self.use_nesterov:\n",
    "                        update = (self.momentum * next_v) + (scaled_lr * grad)\n",
    "                    else:\n",
    "                        update = next_v\n",
    "\n",
    "                    p.data.add_(-update)\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _use_weight_decay(self, param_name):\n",
    "        \"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"\n",
    "        if not self.weight_decay:\n",
    "            return False\n",
    "        if self.exclude_from_weight_decay:\n",
    "            for r in self.exclude_from_weight_decay:\n",
    "                if re.search(r, param_name) is not None:\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    def _do_layer_adaptation(self, param_name):\n",
    "        \"\"\"Whether to do layer-wise learning rate adaptation for `param_name`.\"\"\"\n",
    "        if self.exclude_from_layer_adaptation:\n",
    "            for r in self.exclude_from_layer_adaptation:\n",
    "                if re.search(r, param_name) is not None:\n",
    "                    return False\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "class SimCLR_Loss(nn.Module):\n",
    "    def __init__(self, batch_size, temperature):\n",
    "        super(SimCLR_Loss, self).__init__() ## Why Imrealun?\n",
    "        self.batch_size = batch_size \n",
    "        self.temperature = temperature \n",
    "\n",
    "        self.mask = self.mask_correlated_samples(batch_size)\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\") # For what? Is this CE loss? Internally simCLR loss uses CE\n",
    "        self.similarity_fn = nn.CosineSimilarity(dim=2)\n",
    "\n",
    "    def mask_correlated_samples(self, batch_size):\n",
    "        \"\"\"\n",
    "        We want to create positive and negative pairs\n",
    "        positive pairs come from the augmentations of the same image\n",
    "        negative pairs cant contain augmented version of same image\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        Within a batch, for every image, the 2(N-1) images i.e., excluding 2 versions of its augmented pair\n",
    "        is a negative pair\n",
    "        \"\"\"\n",
    "        \n",
    "        N = 2*batch_size # Each data point gets 2 augmentations\n",
    "        mask = torch.ones((N,N), dtype=bool)\n",
    "        mask = mask.fill_diagonal_(0) # A diagonal matrix of 0 as components\n",
    "\n",
    "        # Going through 0 to batch_size but working in 2 dimensions\n",
    "        for i in range(batch_size):\n",
    "            mask[i, batch_size + i] = 0 # 5, 5+N ; this should not be a pair\n",
    "            mask[batch_size+ i, i] = 0 # 5+N, 5\n",
    "        # Just return a diagonal matrix that masks augmented versions that came from same image\n",
    "        return mask\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        \"\"\"\n",
    "        The Loss function in the paper is given between each data pair, is it?\n",
    "        But here, we deal with a batch\n",
    "        z_i = batch of images with augmented versions 1 (which may all be different)\n",
    "        z_j = batch of images with second version of aug\n",
    "        \"\"\"\n",
    "        N = 2 * self.batch_size \n",
    "        z = torch.cat((z_i, z_j), dim =0) # The 2 augmented images concat to 1 \n",
    "\n",
    "        # Similarity between 2 augmented versions, must be a 2d matrix \n",
    "        sim = self.similarity_fn(z.unsqueeze(1), z.unsqueeze(0))/ self.temperature\n",
    "\n",
    "        # torch.diag (if 1d, inputs are diagonal elemets) \n",
    "        # get just the diagonal of similarity matrix, diagonal_no = self.batch_size \n",
    "        sim_i_j = torch.diag(sim, self.batch_size)\n",
    "        sim_j_i = torch.diag(sim, -self.batch_size)\n",
    "\n",
    "        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N,1) #??\n",
    "        negative_samples = sim[self.mask].reshape(N, -1)\n",
    "\n",
    "        labels = torch.from_numpy(np.array([0]*N)).reshape(-1).to(positive_samples.device).long() #This is strange, positive_samples.device\n",
    "\n",
    "        logits = torch.cat((positive_samples, negative_samples), dim =1)\n",
    "\n",
    "        loss = self.criterion(logits, labels) # CE ?\n",
    "        loss /= N\n",
    "        return loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class Identity(nn.Module):\n",
    "    \"\"\"\n",
    "    Identity Mapping\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class LinearLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Linear Layer and whether or not to use BN 1D\n",
    "    This is used by the projection head below\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, use_bias = True, use_bn = False, **kwargs):\n",
    "        super(LinearLayer, self).__init__(**kwargs)\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features \n",
    "        self.use_bias = use_bias \n",
    "        self.use_bn = use_bn\n",
    "\n",
    "        self.linear = nn.Linear(self.in_features, self.out_features, bias = self.use_bias and not self.use_bn)\n",
    "        if self.use_bn:\n",
    "            self.bn = nn.BatchNorm1d(self.out_features)\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        if self.use_bn:\n",
    "            x = self.bn(x)\n",
    "\n",
    "        return x \n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Projection could be a linear or non linear mapping\n",
    "    hidden features ? No. of features in the intermediate layers\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features, out_features, head_type= 'nonlinear', **kwargs):\n",
    "        super(ProjectionHead, self).__init__(**kwargs)\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.hidden_features = hidden_features \n",
    "        self.head_type = head_type \n",
    "\n",
    "        if self.head_type == 'linear':\n",
    "            self.layers = LinearLayer(self.in_features, self.out_features, False, True)\n",
    "\n",
    "        elif self.head_type == 'nonlinear':\n",
    "            self.layers = nn.Sequential(\n",
    "                LinearLayer(self.in_features, self.hidden_features, True, True),\n",
    "                nn.ReLU(), \n",
    "                LinearLayer(self.hidden_features, self.out_features, False, False)) # Last layer does not use BN\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "class PreModel(nn.Module):\n",
    "    \"\"\"\n",
    "    The model to be used for Pretraining = ResNet50\n",
    "    Plus a projection head on top \n",
    "    \"\"\"\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model \n",
    "\n",
    "        self.encoder = models.resnet50(pretrained= False) ## Default was True, changed to False . Why a pretrained resnet50?\n",
    "\n",
    "        # The input size of this pre-trained may be different (mostly its tuned for imagenet), \n",
    "        # modify it to suit our needs, why do we change this?\n",
    "\n",
    "#         self.encoder.conv1 = nn.Conv2d(3, 64,kernel_size=(3,3), stride = (1,1), bias= False)\n",
    "#         self.encoder.maxpool = Identity() ## Essentially dont do maxpool\n",
    "        self.encoder.fc = nn.Linear(2048,2048) # This also does nothing i.e. identity mapping\n",
    "\n",
    "#         for p in self.encoder.parameters():\n",
    "#             p.requires_grad = True\n",
    "\n",
    "        self.projector = ProjectionHead(2048, 2048, 128) # Because the encoder last fc has output of 1000\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x) # Encoder\n",
    "        #print(out.size())\n",
    "        xp = self.projector(torch.squeeze(out)) # Projection head\n",
    "        return xp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self):\n",
    "\n",
    "        # Arguments\n",
    "        self.seed = 99\n",
    "        self.train_batch_size =  768 # Image sizes larger, so batch size reduced\n",
    "        self.val_batch_size = 768\n",
    "        self.temperature= 0.5\n",
    "        self.train_epochs = 200\n",
    "        self.lr = 1.2 #0.9 #4.8 (0.3 x Batch size )/ 256\n",
    "        self.weight_decay = 1e-6\n",
    "        self.dataset_src ='/home/ubuntu/lambda_data'\n",
    "        self.writer = SummaryWriter()\n",
    "        self.writer = SummaryWriter()\n",
    "        self.logfile =  open('/home/ubuntu/logs/approach_3_logfile.txt', 'w')\n",
    "\n",
    "        print(\"\\n--------------------------------\")\n",
    "        self.logfile.write(\"--------------------------------\")\n",
    "        print(\"Seed: \", self.seed)\n",
    "        self.logfile.write(\"Seed: {}\".format(self.seed))\n",
    "\n",
    "        # Set seeds\n",
    "        random.seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(\"Device Assigned to: \", self.device)\n",
    "        self.logfile.write(\"Device Assigned to: {}\".format(self.device))\n",
    "\n",
    "        ## Data Loading operations\n",
    "        print(\"Data Directory: \", self.dataset_src)\n",
    "        self.logfile.write(\"Data Directory: {}\".format(self.dataset_src))\n",
    "\n",
    "        train_dir = os.path.join(self.dataset_src, \"trainHonda100k/\")\n",
    "        val_dir = os.path.join(self.dataset_src, \"valHonda100k/\")\n",
    "\n",
    "       \n",
    "        self.datagen_train = DataGenerator('train', train_dir)\n",
    "        self.train_dataloader = DataLoader(self.datagen_train, self.train_batch_size,  num_workers = 14, prefetch_factor=8, drop_last = True, shuffle=True)\n",
    "\n",
    "        datagen_val = DataGenerator('val', val_dir)\n",
    "        self.val_dataloader = DataLoader(datagen_val, self.val_batch_size,  num_workers = 14, prefetch_factor=8, drop_last = True)\n",
    "\n",
    "\n",
    "        # datagen_test = DataGenerator('test', val_dir)\n",
    "        # self.test_dataloader = DataLoader(datagen_test, self.val_batch_size, drop_last = True)\n",
    "\n",
    "        print(\"\\nTraining data size: {} X {}\".format(len(self.train_dataloader), self.train_batch_size))\n",
    "        print(\"Validation data size: {} X {}\\n\".format(len(self.val_dataloader), self.val_batch_size))\n",
    "        self.logfile.write(\"Training data size: {} X {}\".format(len(self.train_dataloader), self.train_batch_size))\n",
    "        self.logfile.write(\"Validation data size: {} X {}\\n\".format(len(self.val_dataloader), self.val_batch_size))\n",
    "\n",
    "        self.net = PreModel('resnet50').to(self.device)\n",
    "        print(self.net)\n",
    "        self.logfile.write(\"{}\".format(self.net))\n",
    "\n",
    "        self.criterion = SimCLR_Loss(self.train_batch_size, self.temperature)\n",
    "        self.optimizer = LARS([params for params in self.net.parameters() if params.requires_grad],\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay,\n",
    "            exclude_from_weight_decay=[\"batch_normalization\", \"bias\"],\n",
    "        )\n",
    "\n",
    "\n",
    "        # I removed both of the schedulers as they resulted in some errors being thrown in regards to memory\n",
    "        # I plan to add them back after doing the downstream task, but for now, I just want to get the whole pipeline working\n",
    "\n",
    "        self.warmupscheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lambda epoch: (epoch+1) / 10.0, verbose=True)\n",
    "\n",
    "        # # The default for last_epoch is -1 to begin with so I'm not sure why they specify it here\n",
    "        self.mainscheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optimizer, 500, eta_min=0.05, last_epoch=-1, verbose=True) #500 means number of iterations for first restart\n",
    "\n",
    "        print(\"\\n--------------------------------\")\n",
    "        print(\"Total No. of Trainable Parameters: \",sum(p.numel() for p in self.net.parameters() if p.requires_grad))\n",
    "        self.logfile.write(\"--------------------------------\")\n",
    "        self.logfile.write(\"Total No. of Trainable Parameters: {}\".format(sum(p.numel() for p in self.net.parameters() if p.requires_grad)))\n",
    "\n",
    "    def train(self):\n",
    "        train_loss_collector = np.zeros(self.train_epochs)\n",
    "        val_loss_collector = np.zeros(self.train_epochs) \n",
    "\n",
    "        best_loss = float('inf')\n",
    "        print(\"\\n#### Started Training ####\")\n",
    "        self.logfile.write(\"\\n#### Started Training ####\\n\")\n",
    "\n",
    "        for i in range(self.train_epochs):\n",
    "            self.net.train()\n",
    "\n",
    "            if i==0:\n",
    "                myfile = open('/home/ubuntu/logs/approach_3_model_init_weights.txt', 'w')\n",
    "                myfile.write(\"SEED = %s\\n\" % self.seed)\n",
    "                print(\"Writing Model Initial Weights to a file\\n\")\n",
    "                for param in self.net.parameters():\n",
    "                    myfile.write(\"%s\\n\" % param.data)\n",
    "                myfile.close()\n",
    "\n",
    "            start = time.time()\n",
    "            batch_loss_train = 0 \n",
    "\n",
    "            print(\"Ep. {}/{}:\".format(i, self.train_epochs), end=\"\\t\")\n",
    "            self.logfile.write(\"Ep. {}/{}:\\t\".format(i, self.train_epochs))\n",
    "                            \n",
    "            for bi, (x_i, x_j) in enumerate(self.train_dataloader):\n",
    "                x_i = x_i.squeeze().to(self.device).float()\n",
    "                x_j = x_j.squeeze().to(self.device).float()\n",
    "        \n",
    "                z_i = self.net(x_i)\n",
    "                z_j = self.net(x_i)\n",
    "\n",
    "                self.optimizer.zero_grad() \n",
    "                loss = self.criterion(z_i, z_j)\n",
    "\n",
    "                loss.backward()\n",
    "                self.optimizer.step() \n",
    "\n",
    "                loss_np = loss.cpu().detach().numpy()\n",
    "                self.writer.add_scalar(\"Batch Loss, Train:\", loss_np, bi)\n",
    "\n",
    "                batch_loss_train += loss_np \n",
    "            \n",
    "            if i < 10:\n",
    "                self.warmupscheduler.step()\n",
    "                #self.writer.add_scalar(\"Learning Rate:\", self.warmupscheduler.get_last_lr()[0])\n",
    "            else:\n",
    "                self.mainscheduler.step()\n",
    "                #self.writer.add_scalar(\"Learning Rate:\", self.mainscheduler.get_last_lr()[0])\n",
    "\n",
    "            # Average Batch Loss per epoch\n",
    "            avg_batch_loss_train = batch_loss_train / len(self.train_dataloader)\n",
    "            print(\"Train: ABL {}\".format(round(avg_batch_loss_train,5)), end=\"\\t\")\n",
    "            self.logfile.write(\"Train: ABL {}\".format(round(avg_batch_loss_train,3)))\n",
    "\n",
    "            # Val loss per epoch\n",
    "            avg_batch_loss_val = self.validate(self.net, self.val_dataloader)\n",
    "            val_loss_collector[i] = avg_batch_loss_val\n",
    "\n",
    "            print(\"Val: ABL {},\".format(round(avg_batch_loss_val,3)), end = \"\\t\")\n",
    "            self.logfile.write(\"Val: ABL {}\".format(round(avg_batch_loss_val,3)))\n",
    "\n",
    "            print(\"Time: {} s\\n\".format(round(time.time() - start, 1))) #LR: {}\".format(round(time.time() - start, 1), self.optimizer.param_groups[0]['lr'] )) \n",
    "            self.logfile.write(\"Time: {} s\\n\".format(round(time.time() - start, 1)))\n",
    "\n",
    "            # Generally should be looking at validation loss here but..\n",
    "            if avg_batch_loss_train < best_loss:\n",
    "\n",
    "                best_loss = avg_batch_loss_train\n",
    "                print(\"#### New Model Saved #####\")\n",
    "                self.logfile.write(\"#### New Model Saved #####\\n\")\n",
    "                torch.save(self.net, \"/home/ubuntu/Saved_models/approach_3_trained_model_ep_{}.pt\".format(i))\n",
    "            \n",
    "            elif (i+1)%10 ==0:\n",
    "                print(\"#### New Model Saved #####\")\n",
    "                self.logfile.write(\"#### New Model Saved #####\\n\")\n",
    "                torch.save(self.net, \"/home/ubuntu/Saved_models/approach_3_trained_model_ep_{}.pt\".format(i))\n",
    "                \n",
    "            train_loss_collector[i] = avg_batch_loss_train\n",
    "\n",
    "        self.writer.flush() \n",
    "        self.writer.close()\n",
    "\n",
    "        # Draw loss plot (both train and val)\n",
    "        fig, ax = plt.subplots(figsize=(16,5), dpi = 100)\n",
    "        xticks= np.arange(0,self.train_epochs,50)\n",
    "\n",
    "        ax.set_ylabel(\" Loss (Training & Validation)\")\n",
    "        ax.plot(np.asarray(train_loss_collector))\n",
    "        ax.plot(np.asarray(val_loss_collector)) \n",
    "\n",
    "        ax.set_xticks(xticks) #;\n",
    "        ax.legend([\"Validation\", \"Training\"])\n",
    "        fig.savefig('/home/ubuntu/logs/approach_3_training_result.png')\n",
    "\n",
    "        print(\"#### Ended Training ####\")\n",
    "        self.logfile.write(\"#### Ended Training ####\\n\\n #### Performing Test ####\")\n",
    "\n",
    "        # Just reuse the same code from validate but dont pass model, load the best model from directory\n",
    "        # new_model = PreModel('resnet50').to(self.device)\n",
    "        # new_model.load_state_dict(torch.load('./Saved_models/trained_model.pt', map_location=torch.device('cpu')).state_dict()) # To aviod running out of memory, do this in CPU. Can I flush GPU memory instead?\n",
    "        # avg_batch_loss_test = self.validate(new_model, self.test_dataloader) \n",
    "        # print(\"Test: ABL {},\".format(round(avg_batch_loss_test,3)), end = \"\\t\")\n",
    "        # logfile.write(\"Test: ABL {}\\n\".format(round(avg_batch_loss_test,3)))\n",
    "\n",
    "        self.logfile.close()\n",
    "\n",
    "    def validate(self, current_model, dataloader):\n",
    "\n",
    "        current_model.eval()  \n",
    "        batch_loss_val=0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for bi, (x_i, x_j) in enumerate(dataloader):\n",
    "                x_i = x_i.squeeze().to(self.device).float()\n",
    "                x_j = x_j.squeeze().to(self.device).float()\n",
    "\n",
    "                z_i = self.net(x_i)\n",
    "                z_j = self.net(x_i)\n",
    "\n",
    "                loss = self.criterion(z_i, z_j)\n",
    "                loss_np =  loss.cpu().detach().numpy()\n",
    "                self.writer.add_scalar(\"Regressor: Batch Loss/val\", loss_np, bi)\n",
    "\n",
    "                batch_loss_val +=loss_np\n",
    "  \n",
    "        avg_batch_loss_val = batch_loss_val / len(self.val_dataloader)\n",
    "        return avg_batch_loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------\n",
      "Seed:  99\n",
      "Device Assigned to:  cuda\n",
      "Data Directory:  /home/ubuntu/lambda_data\n",
      "\n",
      "Training data size: 130 X 768\n",
      "Validation data size: 13 X 768\n",
      "\n",
      "PreModel(\n",
      "  (encoder): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (3): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (4): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (5): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): Bottleneck(\n",
      "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): Bottleneck(\n",
      "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  )\n",
      "  (projector): ProjectionHead(\n",
      "    (layers): Sequential(\n",
      "      (0): LinearLayer(\n",
      "        (linear): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "        (bn): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): ReLU()\n",
      "      (2): LinearLayer(\n",
      "        (linear): Linear(in_features=2048, out_features=128, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Adjusting learning rate of group 0 to 9.0000e-02.\n",
      "Epoch     0: adjusting learning rate of group 0 to 9.0000e-01.\n",
      "\n",
      "--------------------------------\n",
      "Total No. of Trainable Parameters:  32164928\n",
      "\n",
      "#### Started Training ####\n",
      "Writing Model Initial Weights to a file\n",
      "\n",
      "Ep. 0/200:\tAdjusting learning rate of group 0 to 1.8000e-01.\n",
      "Train: ABL 5.37565\tVal: ABL 5.816,\tTime: 959.0 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 1/200:\tAdjusting learning rate of group 0 to 2.7000e-01.\n",
      "Train: ABL 5.35449\tVal: ABL 5.419,\tTime: 1036.3 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 2/200:\tAdjusting learning rate of group 0 to 3.6000e-01.\n",
      "Train: ABL 5.35415\tVal: ABL 5.456,\tTime: 1001.8 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 3/200:\tAdjusting learning rate of group 0 to 4.5000e-01.\n",
      "Train: ABL 5.35431\tVal: ABL 5.488,\tTime: 978.6 s\n",
      "\n",
      "Ep. 4/200:\tAdjusting learning rate of group 0 to 5.4000e-01.\n",
      "Train: ABL 5.35452\tVal: ABL 5.564,\tTime: 963.3 s\n",
      "\n",
      "Ep. 5/200:\tAdjusting learning rate of group 0 to 6.3000e-01.\n",
      "Train: ABL 5.35472\tVal: ABL 5.68,\tTime: 964.3 s\n",
      "\n",
      "Ep. 6/200:\tAdjusting learning rate of group 0 to 7.2000e-01.\n",
      "Train: ABL 5.35486\tVal: ABL 5.698,\tTime: 988.0 s\n",
      "\n",
      "Ep. 7/200:\tAdjusting learning rate of group 0 to 8.1000e-01.\n",
      "Train: ABL 5.35501\tVal: ABL 5.849,\tTime: 954.4 s\n",
      "\n",
      "Ep. 8/200:\tAdjusting learning rate of group 0 to 9.0000e-01.\n",
      "Train: ABL 5.3551\tVal: ABL 5.966,\tTime: 956.7 s\n",
      "\n",
      "Ep. 9/200:\tAdjusting learning rate of group 0 to 9.9000e-01.\n",
      "Train: ABL 5.35517\tVal: ABL 6.25,\tTime: 958.0 s\n",
      "\n",
      "Ep. 10/200:\tEpoch     1: adjusting learning rate of group 0 to 8.9999e-01.\n",
      "Train: ABL 5.35525\tVal: ABL 6.287,\tTime: 960.6 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 11/200:\tEpoch     2: adjusting learning rate of group 0 to 8.9997e-01.\n",
      "Train: ABL 5.35489\tVal: ABL 6.24,\tTime: 967.6 s\n",
      "\n",
      "Ep. 12/200:\tEpoch     3: adjusting learning rate of group 0 to 8.9992e-01.\n",
      "Train: ABL 5.3548\tVal: ABL 6.251,\tTime: 950.0 s\n",
      "\n",
      "Ep. 13/200:\tEpoch     4: adjusting learning rate of group 0 to 8.9987e-01.\n",
      "Train: ABL 5.35469\tVal: ABL 6.151,\tTime: 985.7 s\n",
      "\n",
      "Ep. 14/200:\tEpoch     5: adjusting learning rate of group 0 to 8.9979e-01.\n",
      "Train: ABL 5.35463\tVal: ABL 6.087,\tTime: 965.8 s\n",
      "\n",
      "Ep. 15/200:\tEpoch     6: adjusting learning rate of group 0 to 8.9970e-01.\n",
      "Train: ABL 5.35456\tVal: ABL 6.106,\tTime: 967.5 s\n",
      "\n",
      "Ep. 16/200:\tEpoch     7: adjusting learning rate of group 0 to 8.9959e-01.\n",
      "Train: ABL 5.35447\tVal: ABL 5.913,\tTime: 981.0 s\n",
      "\n",
      "Ep. 17/200:\tEpoch     8: adjusting learning rate of group 0 to 8.9946e-01.\n",
      "Train: ABL 5.35442\tVal: ABL 5.865,\tTime: 1042.7 s\n",
      "\n",
      "Ep. 18/200:\tEpoch     9: adjusting learning rate of group 0 to 8.9932e-01.\n",
      "Train: ABL 5.35437\tVal: ABL 5.816,\tTime: 982.7 s\n",
      "\n",
      "Ep. 19/200:\tEpoch    10: adjusting learning rate of group 0 to 8.9916e-01.\n",
      "Train: ABL 5.35432\tVal: ABL 5.742,\tTime: 1001.9 s\n",
      "\n",
      "Ep. 20/200:\tEpoch    11: adjusting learning rate of group 0 to 8.9899e-01.\n",
      "Train: ABL 5.35429\tVal: ABL 5.714,\tTime: 990.4 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 21/200:\tEpoch    12: adjusting learning rate of group 0 to 8.9879e-01.\n",
      "Train: ABL 5.35427\tVal: ABL 5.844,\tTime: 995.1 s\n",
      "\n",
      "Ep. 22/200:\tEpoch    13: adjusting learning rate of group 0 to 8.9858e-01.\n",
      "Train: ABL 5.35424\tVal: ABL 5.815,\tTime: 997.8 s\n",
      "\n",
      "Ep. 23/200:\tEpoch    14: adjusting learning rate of group 0 to 8.9836e-01.\n",
      "Train: ABL 5.3542\tVal: ABL 5.782,\tTime: 988.4 s\n",
      "\n",
      "Ep. 24/200:\tEpoch    15: adjusting learning rate of group 0 to 8.9811e-01.\n",
      "Train: ABL 5.35419\tVal: ABL 5.757,\tTime: 978.8 s\n",
      "\n",
      "Ep. 25/200:\tEpoch    16: adjusting learning rate of group 0 to 8.9785e-01.\n",
      "Train: ABL 5.35419\tVal: ABL 5.717,\tTime: 972.4 s\n",
      "\n",
      "Ep. 26/200:\tEpoch    17: adjusting learning rate of group 0 to 8.9758e-01.\n",
      "Train: ABL 5.35414\tVal: ABL 5.727,\tTime: 1018.2 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 27/200:\tEpoch    18: adjusting learning rate of group 0 to 8.9728e-01.\n",
      "Train: ABL 5.35413\tVal: ABL 5.706,\tTime: 993.7 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 28/200:\tEpoch    19: adjusting learning rate of group 0 to 8.9698e-01.\n",
      "Train: ABL 5.35414\tVal: ABL 5.663,\tTime: 1001.6 s\n",
      "\n",
      "Ep. 29/200:\tEpoch    20: adjusting learning rate of group 0 to 8.9665e-01.\n",
      "Train: ABL 5.3541\tVal: ABL 5.652,\tTime: 995.3 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 30/200:\tEpoch    21: adjusting learning rate of group 0 to 8.9631e-01.\n",
      "Train: ABL 5.3541\tVal: ABL 5.608,\tTime: 1057.1 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 31/200:\tEpoch    22: adjusting learning rate of group 0 to 8.9595e-01.\n",
      "Train: ABL 5.35409\tVal: ABL 5.702,\tTime: 1022.8 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 32/200:\tEpoch    23: adjusting learning rate of group 0 to 8.9557e-01.\n",
      "Train: ABL 5.35407\tVal: ABL 5.657,\tTime: 1011.3 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 33/200:\tEpoch    24: adjusting learning rate of group 0 to 8.9518e-01.\n",
      "Train: ABL 5.35407\tVal: ABL 5.636,\tTime: 1043.4 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 34/200:\tEpoch    25: adjusting learning rate of group 0 to 8.9477e-01.\n",
      "Train: ABL 5.35405\tVal: ABL 5.657,\tTime: 985.9 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 35/200:\tEpoch    26: adjusting learning rate of group 0 to 8.9434e-01.\n",
      "Train: ABL 5.35405\tVal: ABL 5.713,\tTime: 987.9 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 36/200:\tEpoch    27: adjusting learning rate of group 0 to 8.9390e-01.\n",
      "Train: ABL 5.35403\tVal: ABL 5.659,\tTime: 985.5 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 37/200:\tEpoch    28: adjusting learning rate of group 0 to 8.9344e-01.\n",
      "Train: ABL 5.35401\tVal: ABL 5.638,\tTime: 988.3 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 38/200:\tEpoch    29: adjusting learning rate of group 0 to 8.9296e-01.\n",
      "Train: ABL 5.35401\tVal: ABL 5.617,\tTime: 982.7 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 39/200:\tEpoch    30: adjusting learning rate of group 0 to 8.9247e-01.\n",
      "Train: ABL 5.354\tVal: ABL 5.57,\tTime: 990.2 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 40/200:\tEpoch    31: adjusting learning rate of group 0 to 8.9196e-01.\n",
      "Train: ABL 5.35399\tVal: ABL 5.616,\tTime: 995.2 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 41/200:\tEpoch    32: adjusting learning rate of group 0 to 8.9144e-01.\n",
      "Train: ABL 5.35398\tVal: ABL 5.608,\tTime: 1002.1 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 42/200:\tEpoch    33: adjusting learning rate of group 0 to 8.9090e-01.\n",
      "Train: ABL 5.35398\tVal: ABL 5.591,\tTime: 984.2 s\n",
      "\n",
      "Ep. 43/200:\tEpoch    34: adjusting learning rate of group 0 to 8.9034e-01.\n",
      "Train: ABL 5.35398\tVal: ABL 5.681,\tTime: 1046.2 s\n",
      "\n",
      "Ep. 44/200:\tEpoch    35: adjusting learning rate of group 0 to 8.8976e-01.\n",
      "Train: ABL 5.35399\tVal: ABL 5.64,\tTime: 1006.8 s\n",
      "\n",
      "Ep. 45/200:\tEpoch    36: adjusting learning rate of group 0 to 8.8917e-01.\n",
      "Train: ABL 5.35396\tVal: ABL 5.557,\tTime: 1005.6 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 46/200:\tEpoch    37: adjusting learning rate of group 0 to 8.8857e-01.\n",
      "Train: ABL 5.35396\tVal: ABL 5.613,\tTime: 994.6 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 47/200:\tEpoch    38: adjusting learning rate of group 0 to 8.8794e-01.\n",
      "Train: ABL 5.35395\tVal: ABL 5.599,\tTime: 1002.5 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 48/200:\tEpoch    39: adjusting learning rate of group 0 to 8.8730e-01.\n",
      "Train: ABL 5.35393\tVal: ABL 5.57,\tTime: 980.5 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 49/200:\tEpoch    40: adjusting learning rate of group 0 to 8.8665e-01.\n",
      "Train: ABL 5.35393\tVal: ABL 5.559,\tTime: 1005.8 s\n",
      "\n",
      "Ep. 50/200:\tEpoch    41: adjusting learning rate of group 0 to 8.8598e-01.\n",
      "Train: ABL 5.35394\tVal: ABL 5.533,\tTime: 988.1 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 51/200:\tEpoch    42: adjusting learning rate of group 0 to 8.8529e-01.\n",
      "Train: ABL 5.35393\tVal: ABL 5.589,\tTime: 999.8 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 52/200:\tEpoch    43: adjusting learning rate of group 0 to 8.8458e-01.\n",
      "Train: ABL 5.35393\tVal: ABL 5.573,\tTime: 991.2 s\n",
      "\n",
      "Ep. 53/200:\tEpoch    44: adjusting learning rate of group 0 to 8.8386e-01.\n",
      "Train: ABL 5.35392\tVal: ABL 5.591,\tTime: 975.7 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 54/200:\tEpoch    45: adjusting learning rate of group 0 to 8.8312e-01.\n",
      "Train: ABL 5.35393\tVal: ABL 5.544,\tTime: 977.9 s\n",
      "\n",
      "Ep. 55/200:\tEpoch    46: adjusting learning rate of group 0 to 8.8237e-01.\n",
      "Train: ABL 5.35391\tVal: ABL 5.544,\tTime: 982.0 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 56/200:\tEpoch    47: adjusting learning rate of group 0 to 8.8160e-01.\n",
      "Train: ABL 5.3539\tVal: ABL 5.574,\tTime: 985.8 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 57/200:\tEpoch    48: adjusting learning rate of group 0 to 8.8082e-01.\n",
      "Train: ABL 5.35389\tVal: ABL 5.573,\tTime: 1013.8 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 58/200:\tEpoch    49: adjusting learning rate of group 0 to 8.8002e-01.\n",
      "Train: ABL 5.35389\tVal: ABL 5.565,\tTime: 1012.8 s\n",
      "\n",
      "Ep. 59/200:\tEpoch    50: adjusting learning rate of group 0 to 8.7920e-01.\n",
      "Train: ABL 5.3539\tVal: ABL 5.549,\tTime: 981.5 s\n",
      "\n",
      "Ep. 60/200:\tEpoch    51: adjusting learning rate of group 0 to 8.7837e-01.\n",
      "Train: ABL 5.35389\tVal: ABL 5.544,\tTime: 995.0 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 61/200:\tEpoch    52: adjusting learning rate of group 0 to 8.7752e-01.\n",
      "Train: ABL 5.35389\tVal: ABL 5.505,\tTime: 1006.2 s\n",
      "\n",
      "Ep. 62/200:\tEpoch    53: adjusting learning rate of group 0 to 8.7665e-01.\n",
      "Train: ABL 5.3539\tVal: ABL 5.558,\tTime: 993.3 s\n",
      "\n",
      "Ep. 63/200:\tEpoch    54: adjusting learning rate of group 0 to 8.7577e-01.\n",
      "Train: ABL 5.35389\tVal: ABL 5.531,\tTime: 990.4 s\n",
      "\n",
      "Ep. 64/200:\tEpoch    55: adjusting learning rate of group 0 to 8.7487e-01.\n",
      "Train: ABL 5.35389\tVal: ABL 5.551,\tTime: 985.9 s\n",
      "\n",
      "Ep. 65/200:\tEpoch    56: adjusting learning rate of group 0 to 8.7396e-01.\n",
      "Train: ABL 5.35389\tVal: ABL 5.61,\tTime: 1010.7 s\n",
      "\n",
      "Ep. 66/200:\tEpoch    57: adjusting learning rate of group 0 to 8.7303e-01.\n",
      "Train: ABL 5.35389\tVal: ABL 5.555,\tTime: 991.9 s\n",
      "\n",
      "Ep. 67/200:\tEpoch    58: adjusting learning rate of group 0 to 8.7209e-01.\n",
      "Train: ABL 5.35389\tVal: ABL 5.538,\tTime: 981.5 s\n",
      "\n",
      "Ep. 68/200:\tEpoch    59: adjusting learning rate of group 0 to 8.7113e-01.\n",
      "Train: ABL 5.35387\tVal: ABL 5.466,\tTime: 1021.0 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 69/200:\tEpoch    60: adjusting learning rate of group 0 to 8.7016e-01.\n",
      "Train: ABL 5.35386\tVal: ABL 5.567,\tTime: 973.4 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 70/200:\tEpoch    61: adjusting learning rate of group 0 to 8.6916e-01.\n",
      "Train: ABL 5.35386\tVal: ABL 5.553,\tTime: 982.9 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 71/200:\tEpoch    62: adjusting learning rate of group 0 to 8.6816e-01.\n",
      "Train: ABL 5.35385\tVal: ABL 5.551,\tTime: 1012.5 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 72/200:\tEpoch    63: adjusting learning rate of group 0 to 8.6714e-01.\n",
      "Train: ABL 5.35386\tVal: ABL 5.527,\tTime: 1028.0 s\n",
      "\n",
      "Ep. 73/200:\tEpoch    64: adjusting learning rate of group 0 to 8.6610e-01.\n",
      "Train: ABL 5.35388\tVal: ABL 5.516,\tTime: 981.7 s\n",
      "\n",
      "Ep. 74/200:\tEpoch    65: adjusting learning rate of group 0 to 8.6505e-01.\n",
      "Train: ABL 5.35386\tVal: ABL 5.529,\tTime: 964.5 s\n",
      "\n",
      "Ep. 75/200:\tEpoch    66: adjusting learning rate of group 0 to 8.6398e-01.\n",
      "Train: ABL 5.35387\tVal: ABL 5.533,\tTime: 1010.3 s\n",
      "\n",
      "Ep. 76/200:\tEpoch    67: adjusting learning rate of group 0 to 8.6289e-01.\n",
      "Train: ABL 5.35385\tVal: ABL 5.525,\tTime: 1017.6 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 77/200:\tEpoch    68: adjusting learning rate of group 0 to 8.6180e-01.\n",
      "Train: ABL 5.35384\tVal: ABL 5.515,\tTime: 992.9 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 78/200:\tEpoch    69: adjusting learning rate of group 0 to 8.6068e-01.\n",
      "Train: ABL 5.35384\tVal: ABL 5.496,\tTime: 965.8 s\n",
      "\n",
      "Ep. 79/200:\tEpoch    70: adjusting learning rate of group 0 to 8.5955e-01.\n",
      "Train: ABL 5.35385\tVal: ABL 5.532,\tTime: 971.5 s\n",
      "\n",
      "Ep. 80/200:\tEpoch    71: adjusting learning rate of group 0 to 8.5841e-01.\n",
      "Train: ABL 5.35385\tVal: ABL 5.521,\tTime: 987.8 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 81/200:\tEpoch    72: adjusting learning rate of group 0 to 8.5725e-01.\n",
      "Train: ABL 5.35386\tVal: ABL 5.537,\tTime: 975.1 s\n",
      "\n",
      "Ep. 82/200:\tEpoch    73: adjusting learning rate of group 0 to 8.5607e-01.\n",
      "Train: ABL 5.35384\tVal: ABL 5.508,\tTime: 958.3 s\n",
      "\n",
      "Ep. 83/200:\tEpoch    74: adjusting learning rate of group 0 to 8.5488e-01.\n",
      "Train: ABL 5.35384\tVal: ABL 5.513,\tTime: 984.6 s\n",
      "\n",
      "Ep. 84/200:\tEpoch    75: adjusting learning rate of group 0 to 8.5368e-01.\n",
      "Train: ABL 5.35384\tVal: ABL 5.52,\tTime: 999.8 s\n",
      "\n",
      "Ep. 85/200:\tEpoch    76: adjusting learning rate of group 0 to 8.5246e-01.\n",
      "Train: ABL 5.35383\tVal: ABL 5.523,\tTime: 958.7 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 86/200:\tEpoch    77: adjusting learning rate of group 0 to 8.5122e-01.\n",
      "Train: ABL 5.35383\tVal: ABL 5.505,\tTime: 983.6 s\n",
      "\n",
      "Ep. 87/200:\tEpoch    78: adjusting learning rate of group 0 to 8.4997e-01.\n",
      "Train: ABL 5.35382\tVal: ABL 5.548,\tTime: 960.7 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 88/200:\tEpoch    79: adjusting learning rate of group 0 to 8.4871e-01.\n",
      "Train: ABL 5.35382\tVal: ABL 5.545,\tTime: 975.8 s\n",
      "\n",
      "Ep. 89/200:\tEpoch    80: adjusting learning rate of group 0 to 8.4743e-01.\n",
      "Train: ABL 5.35382\tVal: ABL 5.497,\tTime: 986.8 s\n",
      "\n",
      "Ep. 90/200:\tEpoch    81: adjusting learning rate of group 0 to 8.4614e-01.\n",
      "Train: ABL 5.35382\tVal: ABL 5.519,\tTime: 987.9 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 91/200:\tEpoch    82: adjusting learning rate of group 0 to 8.4483e-01.\n",
      "Train: ABL 5.35383\tVal: ABL 5.507,\tTime: 1028.9 s\n",
      "\n",
      "Ep. 92/200:\tEpoch    83: adjusting learning rate of group 0 to 8.4351e-01.\n",
      "Train: ABL 5.35382\tVal: ABL 5.516,\tTime: 1011.7 s\n",
      "\n",
      "Ep. 93/200:\tEpoch    84: adjusting learning rate of group 0 to 8.4217e-01.\n",
      "Train: ABL 5.35383\tVal: ABL 5.524,\tTime: 1048.9 s\n",
      "\n",
      "Ep. 94/200:\tEpoch    85: adjusting learning rate of group 0 to 8.4082e-01.\n",
      "Train: ABL 5.3538\tVal: ABL 5.511,\tTime: 1021.3 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 95/200:\tEpoch    86: adjusting learning rate of group 0 to 8.3945e-01.\n",
      "Train: ABL 5.35381\tVal: ABL 5.514,\tTime: 1032.3 s\n",
      "\n",
      "Ep. 96/200:\tEpoch    87: adjusting learning rate of group 0 to 8.3807e-01.\n",
      "Train: ABL 5.35381\tVal: ABL 5.481,\tTime: 1058.8 s\n",
      "\n",
      "Ep. 97/200:\tEpoch    88: adjusting learning rate of group 0 to 8.3667e-01.\n",
      "Train: ABL 5.3538\tVal: ABL 5.488,\tTime: 1014.9 s\n",
      "\n",
      "Ep. 98/200:\tEpoch    89: adjusting learning rate of group 0 to 8.3526e-01.\n",
      "Train: ABL 5.35379\tVal: ABL 5.508,\tTime: 998.2 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 99/200:\tEpoch    90: adjusting learning rate of group 0 to 8.3384e-01.\n",
      "Train: ABL 5.3538\tVal: ABL 5.526,\tTime: 951.6 s\n",
      "\n",
      "Ep. 100/200:\tEpoch    91: adjusting learning rate of group 0 to 8.3240e-01.\n",
      "Train: ABL 5.35379\tVal: ABL 5.479,\tTime: 967.2 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 101/200:\tEpoch    92: adjusting learning rate of group 0 to 8.3095e-01.\n",
      "Train: ABL 5.35378\tVal: ABL 5.483,\tTime: 998.4 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 102/200:\tEpoch    93: adjusting learning rate of group 0 to 8.2948e-01.\n",
      "Train: ABL 5.35378\tVal: ABL 5.533,\tTime: 1012.8 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 103/200:\tEpoch    94: adjusting learning rate of group 0 to 8.2800e-01.\n",
      "Train: ABL 5.35378\tVal: ABL 5.472,\tTime: 1038.3 s\n",
      "\n",
      "Ep. 104/200:\tEpoch    95: adjusting learning rate of group 0 to 8.2651e-01.\n",
      "Train: ABL 5.35378\tVal: ABL 5.503,\tTime: 1057.3 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 105/200:\tEpoch    96: adjusting learning rate of group 0 to 8.2500e-01.\n",
      "Train: ABL 5.35378\tVal: ABL 5.505,\tTime: 1047.0 s\n",
      "\n",
      "Ep. 106/200:\tEpoch    97: adjusting learning rate of group 0 to 8.2348e-01.\n",
      "Train: ABL 5.35379\tVal: ABL 5.518,\tTime: 1019.6 s\n",
      "\n",
      "Ep. 107/200:\tEpoch    98: adjusting learning rate of group 0 to 8.2194e-01.\n",
      "Train: ABL 5.35378\tVal: ABL 5.487,\tTime: 1074.1 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 108/200:\tEpoch    99: adjusting learning rate of group 0 to 8.2040e-01.\n",
      "Train: ABL 5.35378\tVal: ABL 5.453,\tTime: 1051.3 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 109/200:\tEpoch   100: adjusting learning rate of group 0 to 8.1883e-01.\n",
      "Train: ABL 5.35377\tVal: ABL 5.487,\tTime: 1025.2 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 110/200:\tEpoch   101: adjusting learning rate of group 0 to 8.1726e-01.\n",
      "Train: ABL 5.35379\tVal: ABL 5.466,\tTime: 1030.1 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 111/200:\tEpoch   102: adjusting learning rate of group 0 to 8.1567e-01.\n",
      "Train: ABL 5.3538\tVal: ABL 5.464,\tTime: 1026.0 s\n",
      "\n",
      "Ep. 112/200:\tEpoch   103: adjusting learning rate of group 0 to 8.1406e-01.\n",
      "Train: ABL 5.35379\tVal: ABL 5.499,\tTime: 1021.8 s\n",
      "\n",
      "Ep. 113/200:\tEpoch   104: adjusting learning rate of group 0 to 8.1245e-01.\n",
      "Train: ABL 5.35378\tVal: ABL 5.482,\tTime: 1019.9 s\n",
      "\n",
      "Ep. 114/200:\tEpoch   105: adjusting learning rate of group 0 to 8.1082e-01.\n",
      "Train: ABL 5.3538\tVal: ABL 5.478,\tTime: 1012.7 s\n",
      "\n",
      "Ep. 115/200:\tEpoch   106: adjusting learning rate of group 0 to 8.0917e-01.\n",
      "Train: ABL 5.35378\tVal: ABL 5.48,\tTime: 1019.7 s\n",
      "\n",
      "Ep. 116/200:\tEpoch   107: adjusting learning rate of group 0 to 8.0752e-01.\n",
      "Train: ABL 5.35378\tVal: ABL 5.442,\tTime: 973.0 s\n",
      "\n",
      "Ep. 117/200:\tEpoch   108: adjusting learning rate of group 0 to 8.0585e-01.\n",
      "Train: ABL 5.35377\tVal: ABL 5.449,\tTime: 980.4 s\n",
      "\n",
      "Ep. 118/200:\tEpoch   109: adjusting learning rate of group 0 to 8.0416e-01.\n",
      "Train: ABL 5.3538\tVal: ABL 5.467,\tTime: 996.0 s\n",
      "\n",
      "Ep. 119/200:\tEpoch   110: adjusting learning rate of group 0 to 8.0247e-01.\n",
      "Train: ABL 5.35377\tVal: ABL 5.477,\tTime: 978.5 s\n",
      "\n",
      "Ep. 120/200:\tEpoch   111: adjusting learning rate of group 0 to 8.0076e-01.\n",
      "Train: ABL 5.35377\tVal: ABL 5.492,\tTime: 979.9 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 121/200:\tEpoch   112: adjusting learning rate of group 0 to 7.9904e-01.\n",
      "Train: ABL 5.35379\tVal: ABL 5.449,\tTime: 1005.6 s\n",
      "\n",
      "Ep. 122/200:\tEpoch   113: adjusting learning rate of group 0 to 7.9730e-01.\n",
      "Train: ABL 5.35379\tVal: ABL 5.498,\tTime: 998.0 s\n",
      "\n",
      "Ep. 123/200:\tEpoch   114: adjusting learning rate of group 0 to 7.9556e-01.\n",
      "Train: ABL 5.35381\tVal: ABL 5.502,\tTime: 1027.6 s\n",
      "\n",
      "Ep. 124/200:\tEpoch   115: adjusting learning rate of group 0 to 7.9380e-01.\n",
      "Train: ABL 5.35378\tVal: ABL 5.463,\tTime: 1019.4 s\n",
      "\n",
      "Ep. 125/200:\tEpoch   116: adjusting learning rate of group 0 to 7.9202e-01.\n",
      "Train: ABL 5.35376\tVal: ABL 5.446,\tTime: 1017.8 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 126/200:\tEpoch   117: adjusting learning rate of group 0 to 7.9024e-01.\n",
      "Train: ABL 5.35376\tVal: ABL 5.476,\tTime: 989.7 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 127/200:\tEpoch   118: adjusting learning rate of group 0 to 7.8844e-01.\n",
      "Train: ABL 5.35377\tVal: ABL 5.464,\tTime: 984.0 s\n",
      "\n",
      "Ep. 128/200:\tEpoch   119: adjusting learning rate of group 0 to 7.8663e-01.\n",
      "Train: ABL 5.35378\tVal: ABL 5.475,\tTime: 1000.6 s\n",
      "\n",
      "Ep. 129/200:\tEpoch   120: adjusting learning rate of group 0 to 7.8481e-01.\n",
      "Train: ABL 5.35376\tVal: ABL 5.454,\tTime: 1021.1 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 130/200:\tEpoch   121: adjusting learning rate of group 0 to 7.8298e-01.\n",
      "Train: ABL 5.35374\tVal: ABL 5.455,\tTime: 1042.5 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 131/200:\tEpoch   122: adjusting learning rate of group 0 to 7.8113e-01.\n",
      "Train: ABL 5.35375\tVal: ABL 5.453,\tTime: 1048.5 s\n",
      "\n",
      "Ep. 132/200:\tEpoch   123: adjusting learning rate of group 0 to 7.7927e-01.\n",
      "Train: ABL 5.35376\tVal: ABL 5.476,\tTime: 1027.1 s\n",
      "\n",
      "Ep. 133/200:\tEpoch   124: adjusting learning rate of group 0 to 7.7740e-01.\n",
      "Train: ABL 5.35375\tVal: ABL 5.438,\tTime: 1032.8 s\n",
      "\n",
      "Ep. 134/200:\tEpoch   125: adjusting learning rate of group 0 to 7.7552e-01.\n",
      "Train: ABL 5.35375\tVal: ABL 5.474,\tTime: 1045.5 s\n",
      "\n",
      "Ep. 135/200:\tEpoch   126: adjusting learning rate of group 0 to 7.7363e-01.\n",
      "Train: ABL 5.35375\tVal: ABL 5.485,\tTime: 1053.7 s\n",
      "\n",
      "Ep. 136/200:\tEpoch   127: adjusting learning rate of group 0 to 7.7172e-01.\n",
      "Train: ABL 5.35375\tVal: ABL 5.483,\tTime: 1037.5 s\n",
      "\n",
      "Ep. 137/200:\tEpoch   128: adjusting learning rate of group 0 to 7.6980e-01.\n",
      "Train: ABL 5.35375\tVal: ABL 5.457,\tTime: 1007.9 s\n",
      "\n",
      "Ep. 138/200:\tEpoch   129: adjusting learning rate of group 0 to 7.6787e-01.\n",
      "Train: ABL 5.35372\tVal: ABL 5.47,\tTime: 1004.8 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 139/200:\tEpoch   130: adjusting learning rate of group 0 to 7.6593e-01.\n",
      "Train: ABL 5.35375\tVal: ABL 5.495,\tTime: 1017.9 s\n",
      "\n",
      "Ep. 140/200:\tEpoch   131: adjusting learning rate of group 0 to 7.6398e-01.\n",
      "Train: ABL 5.35373\tVal: ABL 5.502,\tTime: 973.3 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 141/200:\tEpoch   132: adjusting learning rate of group 0 to 7.6202e-01.\n",
      "Train: ABL 5.35374\tVal: ABL 5.441,\tTime: 1025.7 s\n",
      "\n",
      "Ep. 142/200:\tEpoch   133: adjusting learning rate of group 0 to 7.6004e-01.\n",
      "Train: ABL 5.35375\tVal: ABL 5.456,\tTime: 994.2 s\n",
      "\n",
      "Ep. 143/200:\tEpoch   134: adjusting learning rate of group 0 to 7.5806e-01.\n",
      "Train: ABL 5.35374\tVal: ABL 5.482,\tTime: 1040.6 s\n",
      "\n",
      "Ep. 144/200:\tEpoch   135: adjusting learning rate of group 0 to 7.5606e-01.\n",
      "Train: ABL 5.35373\tVal: ABL 5.49,\tTime: 1029.0 s\n",
      "\n",
      "Ep. 145/200:\tEpoch   136: adjusting learning rate of group 0 to 7.5405e-01.\n",
      "Train: ABL 5.35373\tVal: ABL 5.474,\tTime: 1020.7 s\n",
      "\n",
      "Ep. 146/200:\tEpoch   137: adjusting learning rate of group 0 to 7.5203e-01.\n",
      "Train: ABL 5.35373\tVal: ABL 5.448,\tTime: 1013.4 s\n",
      "\n",
      "Ep. 147/200:\tEpoch   138: adjusting learning rate of group 0 to 7.5000e-01.\n",
      "Train: ABL 5.35372\tVal: ABL 5.478,\tTime: 1011.6 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 148/200:\tEpoch   139: adjusting learning rate of group 0 to 7.4796e-01.\n",
      "Train: ABL 5.35373\tVal: ABL 5.468,\tTime: 996.1 s\n",
      "\n",
      "Ep. 149/200:\tEpoch   140: adjusting learning rate of group 0 to 7.4591e-01.\n",
      "Train: ABL 5.35374\tVal: ABL 5.456,\tTime: 1066.2 s\n",
      "\n",
      "Ep. 150/200:\tEpoch   141: adjusting learning rate of group 0 to 7.4384e-01.\n",
      "Train: ABL 5.35372\tVal: ABL 5.484,\tTime: 1025.1 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 151/200:\tEpoch   142: adjusting learning rate of group 0 to 7.4177e-01.\n",
      "Train: ABL 5.35373\tVal: ABL 5.446,\tTime: 1027.1 s\n",
      "\n",
      "Ep. 152/200:\tEpoch   143: adjusting learning rate of group 0 to 7.3968e-01.\n",
      "Train: ABL 5.35374\tVal: ABL 5.469,\tTime: 1022.3 s\n",
      "\n",
      "Ep. 153/200:\tEpoch   144: adjusting learning rate of group 0 to 7.3759e-01.\n",
      "Train: ABL 5.35374\tVal: ABL 5.487,\tTime: 1016.7 s\n",
      "\n",
      "Ep. 154/200:\tEpoch   145: adjusting learning rate of group 0 to 7.3549e-01.\n",
      "Train: ABL 5.35376\tVal: ABL 5.453,\tTime: 1056.7 s\n",
      "\n",
      "Ep. 155/200:\tEpoch   146: adjusting learning rate of group 0 to 7.3337e-01.\n",
      "Train: ABL 5.35373\tVal: ABL 5.474,\tTime: 1043.2 s\n",
      "\n",
      "Ep. 156/200:\tEpoch   147: adjusting learning rate of group 0 to 7.3125e-01.\n",
      "Train: ABL 5.35371\tVal: ABL 5.433,\tTime: 1044.4 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 157/200:\tEpoch   148: adjusting learning rate of group 0 to 7.2911e-01.\n",
      "Train: ABL 5.35372\tVal: ABL 5.429,\tTime: 995.9 s\n",
      "\n",
      "Ep. 158/200:\tEpoch   149: adjusting learning rate of group 0 to 7.2696e-01.\n",
      "Train: ABL 5.35371\tVal: ABL 5.437,\tTime: 1003.4 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 159/200:\tEpoch   150: adjusting learning rate of group 0 to 7.2481e-01.\n",
      "Train: ABL 5.35371\tVal: ABL 5.471,\tTime: 1035.5 s\n",
      "\n",
      "Ep. 160/200:\tEpoch   151: adjusting learning rate of group 0 to 7.2264e-01.\n",
      "Train: ABL 5.35371\tVal: ABL 5.454,\tTime: 1036.0 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 161/200:\tEpoch   152: adjusting learning rate of group 0 to 7.2047e-01.\n",
      "Train: ABL 5.3537\tVal: ABL 5.461,\tTime: 1011.9 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 162/200:\tEpoch   153: adjusting learning rate of group 0 to 7.1828e-01.\n",
      "Train: ABL 5.35371\tVal: ABL 5.425,\tTime: 1014.3 s\n",
      "\n",
      "Ep. 163/200:\tEpoch   154: adjusting learning rate of group 0 to 7.1609e-01.\n",
      "Train: ABL 5.35372\tVal: ABL 5.444,\tTime: 977.5 s\n",
      "\n",
      "Ep. 164/200:\tEpoch   155: adjusting learning rate of group 0 to 7.1389e-01.\n",
      "Train: ABL 5.35372\tVal: ABL 5.452,\tTime: 1031.1 s\n",
      "\n",
      "Ep. 165/200:\tEpoch   156: adjusting learning rate of group 0 to 7.1167e-01.\n",
      "Train: ABL 5.35372\tVal: ABL 5.458,\tTime: 1018.6 s\n",
      "\n",
      "Ep. 166/200:\tEpoch   157: adjusting learning rate of group 0 to 7.0945e-01.\n",
      "Train: ABL 5.35371\tVal: ABL 5.442,\tTime: 1015.0 s\n",
      "\n",
      "Ep. 167/200:\tEpoch   158: adjusting learning rate of group 0 to 7.0722e-01.\n",
      "Train: ABL 5.35371\tVal: ABL 5.464,\tTime: 1029.7 s\n",
      "\n",
      "Ep. 168/200:\tEpoch   159: adjusting learning rate of group 0 to 7.0498e-01.\n",
      "Train: ABL 5.35371\tVal: ABL 5.466,\tTime: 1046.2 s\n",
      "\n",
      "Ep. 169/200:\tEpoch   160: adjusting learning rate of group 0 to 7.0273e-01.\n",
      "Train: ABL 5.35372\tVal: ABL 5.463,\tTime: 1044.4 s\n",
      "\n",
      "Ep. 170/200:\tEpoch   161: adjusting learning rate of group 0 to 7.0047e-01.\n",
      "Train: ABL 5.35369\tVal: ABL 5.447,\tTime: 1007.2 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 171/200:\tEpoch   162: adjusting learning rate of group 0 to 6.9820e-01.\n",
      "Train: ABL 5.35371\tVal: ABL 5.445,\tTime: 977.2 s\n",
      "\n",
      "Ep. 172/200:\tEpoch   163: adjusting learning rate of group 0 to 6.9592e-01.\n",
      "Train: ABL 5.3537\tVal: ABL 5.455,\tTime: 1014.3 s\n",
      "\n",
      "Ep. 173/200:\tEpoch   164: adjusting learning rate of group 0 to 6.9364e-01.\n",
      "Train: ABL 5.3537\tVal: ABL 5.458,\tTime: 999.9 s\n",
      "\n",
      "Ep. 174/200:\tEpoch   165: adjusting learning rate of group 0 to 6.9134e-01.\n",
      "Train: ABL 5.3537\tVal: ABL 5.431,\tTime: 983.6 s\n",
      "\n",
      "Ep. 175/200:\tEpoch   166: adjusting learning rate of group 0 to 6.8904e-01.\n",
      "Train: ABL 5.3537\tVal: ABL 5.422,\tTime: 980.9 s\n",
      "\n",
      "Ep. 176/200:\tEpoch   167: adjusting learning rate of group 0 to 6.8673e-01.\n",
      "Train: ABL 5.35368\tVal: ABL 5.417,\tTime: 981.9 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 177/200:\tEpoch   168: adjusting learning rate of group 0 to 6.8441e-01.\n",
      "Train: ABL 5.3537\tVal: ABL 5.424,\tTime: 980.4 s\n",
      "\n",
      "Ep. 178/200:\tEpoch   169: adjusting learning rate of group 0 to 6.8208e-01.\n",
      "Train: ABL 5.35368\tVal: ABL 5.426,\tTime: 988.9 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 179/200:\tEpoch   170: adjusting learning rate of group 0 to 6.7975e-01.\n",
      "Train: ABL 5.35367\tVal: ABL 5.42,\tTime: 1032.0 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 180/200:\tEpoch   171: adjusting learning rate of group 0 to 6.7740e-01.\n",
      "Train: ABL 5.35369\tVal: ABL 5.419,\tTime: 1021.8 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 181/200:\tEpoch   172: adjusting learning rate of group 0 to 6.7505e-01.\n",
      "Train: ABL 5.35369\tVal: ABL 5.436,\tTime: 1025.2 s\n",
      "\n",
      "Ep. 182/200:\tEpoch   173: adjusting learning rate of group 0 to 6.7269e-01.\n",
      "Train: ABL 5.3537\tVal: ABL 5.438,\tTime: 1029.5 s\n",
      "\n",
      "Ep. 183/200:\tEpoch   174: adjusting learning rate of group 0 to 6.7032e-01.\n",
      "Train: ABL 5.3537\tVal: ABL 5.442,\tTime: 1043.0 s\n",
      "\n",
      "Ep. 184/200:\tEpoch   175: adjusting learning rate of group 0 to 6.6795e-01.\n",
      "Train: ABL 5.3537\tVal: ABL 5.436,\tTime: 987.9 s\n",
      "\n",
      "Ep. 185/200:\tEpoch   176: adjusting learning rate of group 0 to 6.6556e-01.\n",
      "Train: ABL 5.3537\tVal: ABL 5.436,\tTime: 1068.6 s\n",
      "\n",
      "Ep. 186/200:\tEpoch   177: adjusting learning rate of group 0 to 6.6317e-01.\n",
      "Train: ABL 5.35368\tVal: ABL 5.443,\tTime: 1048.9 s\n",
      "\n",
      "Ep. 187/200:\tEpoch   178: adjusting learning rate of group 0 to 6.6077e-01.\n",
      "Train: ABL 5.35369\tVal: ABL 5.445,\tTime: 1029.5 s\n",
      "\n",
      "Ep. 188/200:\tEpoch   179: adjusting learning rate of group 0 to 6.5837e-01.\n",
      "Train: ABL 5.3537\tVal: ABL 5.434,\tTime: 1042.5 s\n",
      "\n",
      "Ep. 189/200:\tEpoch   180: adjusting learning rate of group 0 to 6.5596e-01.\n",
      "Train: ABL 5.35369\tVal: ABL 5.448,\tTime: 1035.0 s\n",
      "\n",
      "Ep. 190/200:\tEpoch   181: adjusting learning rate of group 0 to 6.5354e-01.\n",
      "Train: ABL 5.35368\tVal: ABL 5.446,\tTime: 1000.6 s\n",
      "\n",
      "#### New Model Saved #####\n",
      "Ep. 191/200:\tEpoch   182: adjusting learning rate of group 0 to 6.5111e-01.\n",
      "Train: ABL 5.35369\tVal: ABL 5.451,\tTime: 1034.4 s\n",
      "\n",
      "Ep. 192/200:\tEpoch   183: adjusting learning rate of group 0 to 6.4868e-01.\n",
      "Train: ABL 5.35368\tVal: ABL 5.451,\tTime: 1012.6 s\n",
      "\n",
      "Ep. 193/200:\tEpoch   184: adjusting learning rate of group 0 to 6.4624e-01.\n",
      "Train: ABL 5.35368\tVal: ABL 5.425,\tTime: 976.9 s\n",
      "\n",
      "Ep. 194/200:\tEpoch   185: adjusting learning rate of group 0 to 6.4379e-01.\n",
      "Train: ABL 5.35369\tVal: ABL 5.44,\tTime: 1022.3 s\n",
      "\n",
      "Ep. 195/200:\tEpoch   186: adjusting learning rate of group 0 to 6.4133e-01.\n",
      "Train: ABL 5.35369\tVal: ABL 5.427,\tTime: 1030.2 s\n",
      "\n",
      "Ep. 196/200:\tEpoch   187: adjusting learning rate of group 0 to 6.3887e-01.\n",
      "Train: ABL 5.35369\tVal: ABL 5.463,\tTime: 1034.8 s\n",
      "\n",
      "Ep. 197/200:\tEpoch   188: adjusting learning rate of group 0 to 6.3641e-01.\n",
      "Train: ABL 5.35368\tVal: ABL 5.425,\tTime: 1040.3 s\n",
      "\n",
      "Ep. 198/200:\tEpoch   189: adjusting learning rate of group 0 to 6.3393e-01.\n",
      "Train: ABL 5.35368\tVal: ABL 5.438,\tTime: 1025.4 s\n",
      "\n",
      "Ep. 199/200:\tEpoch   190: adjusting learning rate of group 0 to 6.3145e-01.\n",
      "Train: ABL 5.35368\tVal: ABL 5.422,\tTime: 1010.1 s\n",
      "\n",
      "#### Ended Training ####\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'logfile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5309eb71a9c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-e200505656dc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;31m# logfile.write(\"Test: ABL {}\\n\".format(round(avg_batch_loss_test,3)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mlogfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logfile' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x500 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tr = Trainer()\n",
    "tr.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
