--------------------------------Seed: 99Device Assigned to: cudaData Directory: /home/ubuntu/lambda_dataTraining data size: 130 X 768Validation data size: 13 X 768
PreModel(
  (encoder): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=2048, bias=True)
  )
  (projector): ProjectionHead(
    (layers): Sequential(
      (0): LinearLayer(
        (linear): Linear(in_features=2048, out_features=2048, bias=False)
        (bn): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): ReLU()
      (2): LinearLayer(
        (linear): Linear(in_features=2048, out_features=128, bias=False)
      )
    )
  )
)--------------------------------Total No. of Trainable Parameters: 32164928
#### Started Training ####
Ep. 0/200:	Train: ABL 5.376Val: ABL 5.816Time: 959.0 s
#### New Model Saved #####
Ep. 1/200:	Train: ABL 5.354Val: ABL 5.419Time: 1036.3 s
#### New Model Saved #####
Ep. 2/200:	Train: ABL 5.354Val: ABL 5.456Time: 1001.8 s
#### New Model Saved #####
Ep. 3/200:	Train: ABL 5.354Val: ABL 5.488Time: 978.6 s
Ep. 4/200:	Train: ABL 5.355Val: ABL 5.564Time: 963.3 s
Ep. 5/200:	Train: ABL 5.355Val: ABL 5.68Time: 964.3 s
Ep. 6/200:	Train: ABL 5.355Val: ABL 5.698Time: 988.0 s
Ep. 7/200:	Train: ABL 5.355Val: ABL 5.849Time: 954.4 s
Ep. 8/200:	Train: ABL 5.355Val: ABL 5.966Time: 956.7 s
Ep. 9/200:	Train: ABL 5.355Val: ABL 6.25Time: 958.0 s
Ep. 10/200:	Train: ABL 5.355Val: ABL 6.287Time: 960.6 s
#### New Model Saved #####
Ep. 11/200:	Train: ABL 5.355Val: ABL 6.24Time: 967.6 s
Ep. 12/200:	Train: ABL 5.355Val: ABL 6.251Time: 950.0 s
Ep. 13/200:	Train: ABL 5.355Val: ABL 6.151Time: 985.7 s
Ep. 14/200:	Train: ABL 5.355Val: ABL 6.087Time: 965.8 s
Ep. 15/200:	Train: ABL 5.355Val: ABL 6.106Time: 967.5 s
Ep. 16/200:	Train: ABL 5.354Val: ABL 5.913Time: 981.0 s
Ep. 17/200:	Train: ABL 5.354Val: ABL 5.865Time: 1042.7 s
Ep. 18/200:	Train: ABL 5.354Val: ABL 5.816Time: 982.7 s
Ep. 19/200:	Train: ABL 5.354Val: ABL 5.742Time: 1001.9 s
Ep. 20/200:	Train: ABL 5.354Val: ABL 5.714Time: 990.4 s
#### New Model Saved #####
Ep. 21/200:	Train: ABL 5.354Val: ABL 5.844Time: 995.1 s
Ep. 22/200:	Train: ABL 5.354Val: ABL 5.815Time: 997.8 s
Ep. 23/200:	Train: ABL 5.354Val: ABL 5.782Time: 988.4 s
Ep. 24/200:	Train: ABL 5.354Val: ABL 5.757Time: 978.8 s
Ep. 25/200:	Train: ABL 5.354Val: ABL 5.717Time: 972.4 s
Ep. 26/200:	Train: ABL 5.354Val: ABL 5.727Time: 1018.2 s
#### New Model Saved #####
Ep. 27/200:	Train: ABL 5.354Val: ABL 5.706Time: 993.7 s
#### New Model Saved #####
Ep. 28/200:	Train: ABL 5.354Val: ABL 5.663Time: 1001.6 s
Ep. 29/200:	Train: ABL 5.354Val: ABL 5.652Time: 995.3 s
#### New Model Saved #####
Ep. 30/200:	Train: ABL 5.354Val: ABL 5.608Time: 1057.1 s
#### New Model Saved #####
Ep. 31/200:	Train: ABL 5.354Val: ABL 5.702Time: 1022.8 s
#### New Model Saved #####
Ep. 32/200:	Train: ABL 5.354Val: ABL 5.657Time: 1011.3 s
#### New Model Saved #####
Ep. 33/200:	Train: ABL 5.354Val: ABL 5.636Time: 1043.4 s
#### New Model Saved #####
Ep. 34/200:	Train: ABL 5.354Val: ABL 5.657Time: 985.9 s
#### New Model Saved #####
Ep. 35/200:	Train: ABL 5.354Val: ABL 5.713Time: 987.9 s
#### New Model Saved #####
Ep. 36/200:	Train: ABL 5.354Val: ABL 5.659Time: 985.5 s
#### New Model Saved #####
Ep. 37/200:	Train: ABL 5.354Val: ABL 5.638Time: 988.3 s
#### New Model Saved #####
Ep. 38/200:	Train: ABL 5.354Val: ABL 5.617Time: 982.7 s
#### New Model Saved #####
Ep. 39/200:	Train: ABL 5.354Val: ABL 5.57Time: 990.2 s
#### New Model Saved #####
Ep. 40/200:	Train: ABL 5.354Val: ABL 5.616Time: 995.2 s
#### New Model Saved #####
Ep. 41/200:	Train: ABL 5.354Val: ABL 5.608Time: 1002.1 s
#### New Model Saved #####
Ep. 42/200:	Train: ABL 5.354Val: ABL 5.591Time: 984.2 s
Ep. 43/200:	Train: ABL 5.354Val: ABL 5.681Time: 1046.2 s
Ep. 44/200:	Train: ABL 5.354Val: ABL 5.64Time: 1006.8 s
Ep. 45/200:	Train: ABL 5.354Val: ABL 5.557Time: 1005.6 s
#### New Model Saved #####
Ep. 46/200:	Train: ABL 5.354Val: ABL 5.613Time: 994.6 s
#### New Model Saved #####
Ep. 47/200:	Train: ABL 5.354Val: ABL 5.599Time: 1002.5 s
#### New Model Saved #####
Ep. 48/200:	Train: ABL 5.354Val: ABL 5.57Time: 980.5 s
#### New Model Saved #####
Ep. 49/200:	Train: ABL 5.354Val: ABL 5.559Time: 1005.8 s
Ep. 50/200:	Train: ABL 5.354Val: ABL 5.533Time: 988.1 s
#### New Model Saved #####
Ep. 51/200:	Train: ABL 5.354Val: ABL 5.589Time: 999.8 s
#### New Model Saved #####
Ep. 52/200:	Train: ABL 5.354Val: ABL 5.573Time: 991.2 s
Ep. 53/200:	Train: ABL 5.354Val: ABL 5.591Time: 975.7 s
#### New Model Saved #####
Ep. 54/200:	Train: ABL 5.354Val: ABL 5.544Time: 977.9 s
Ep. 55/200:	Train: ABL 5.354Val: ABL 5.544Time: 982.0 s
#### New Model Saved #####
Ep. 56/200:	Train: ABL 5.354Val: ABL 5.574Time: 985.8 s
#### New Model Saved #####
Ep. 57/200:	Train: ABL 5.354Val: ABL 5.573Time: 1013.8 s
#### New Model Saved #####
Ep. 58/200:	Train: ABL 5.354Val: ABL 5.565Time: 1012.8 s
Ep. 59/200:	Train: ABL 5.354Val: ABL 5.549Time: 981.5 s
Ep. 60/200:	Train: ABL 5.354Val: ABL 5.544Time: 995.0 s
#### New Model Saved #####
Ep. 61/200:	Train: ABL 5.354Val: ABL 5.505Time: 1006.2 s
Ep. 62/200:	Train: ABL 5.354Val: ABL 5.558Time: 993.3 s
Ep. 63/200:	Train: ABL 5.354Val: ABL 5.531Time: 990.4 s
Ep. 64/200:	Train: ABL 5.354Val: ABL 5.551Time: 985.9 s
Ep. 65/200:	Train: ABL 5.354Val: ABL 5.61Time: 1010.7 s
Ep. 66/200:	Train: ABL 5.354Val: ABL 5.555Time: 991.9 s
Ep. 67/200:	Train: ABL 5.354Val: ABL 5.538Time: 981.5 s
Ep. 68/200:	Train: ABL 5.354Val: ABL 5.466Time: 1021.0 s
#### New Model Saved #####
Ep. 69/200:	Train: ABL 5.354Val: ABL 5.567Time: 973.4 s
#### New Model Saved #####
Ep. 70/200:	Train: ABL 5.354Val: ABL 5.553Time: 982.9 s
#### New Model Saved #####
Ep. 71/200:	Train: ABL 5.354Val: ABL 5.551Time: 1012.5 s
#### New Model Saved #####
Ep. 72/200:	Train: ABL 5.354Val: ABL 5.527Time: 1028.0 s
Ep. 73/200:	Train: ABL 5.354Val: ABL 5.516Time: 981.7 s
Ep. 74/200:	Train: ABL 5.354Val: ABL 5.529Time: 964.5 s
Ep. 75/200:	Train: ABL 5.354Val: ABL 5.533Time: 1010.3 s
Ep. 76/200:	Train: ABL 5.354Val: ABL 5.525Time: 1017.6 s
#### New Model Saved #####
Ep. 77/200:	Train: ABL 5.354Val: ABL 5.515Time: 992.9 s
#### New Model Saved #####
Ep. 78/200:	Train: ABL 5.354Val: ABL 5.496Time: 965.8 s
Ep. 79/200:	Train: ABL 5.354Val: ABL 5.532Time: 971.5 s
Ep. 80/200:	Train: ABL 5.354Val: ABL 5.521Time: 987.8 s
#### New Model Saved #####
Ep. 81/200:	Train: ABL 5.354Val: ABL 5.537Time: 975.1 s
Ep. 82/200:	Train: ABL 5.354Val: ABL 5.508Time: 958.3 s
Ep. 83/200:	Train: ABL 5.354Val: ABL 5.513Time: 984.6 s
Ep. 84/200:	Train: ABL 5.354Val: ABL 5.52Time: 999.8 s
Ep. 85/200:	Train: ABL 5.354Val: ABL 5.523Time: 958.7 s
#### New Model Saved #####
Ep. 86/200:	Train: ABL 5.354Val: ABL 5.505Time: 983.6 s
Ep. 87/200:	Train: ABL 5.354Val: ABL 5.548Time: 960.7 s
#### New Model Saved #####
Ep. 88/200:	Train: ABL 5.354Val: ABL 5.545Time: 975.8 s
Ep. 89/200:	Train: ABL 5.354Val: ABL 5.497Time: 986.8 s
Ep. 90/200:	Train: ABL 5.354Val: ABL 5.519Time: 987.9 s
#### New Model Saved #####
Ep. 91/200:	Train: ABL 5.354Val: ABL 5.507Time: 1028.9 s
Ep. 92/200:	Train: ABL 5.354Val: ABL 5.516Time: 1011.7 s
Ep. 93/200:	Train: ABL 5.354Val: ABL 5.524Time: 1048.9 s
Ep. 94/200:	Train: ABL 5.354Val: ABL 5.511Time: 1021.3 s
#### New Model Saved #####
Ep. 95/200:	Train: ABL 5.354Val: ABL 5.514Time: 1032.3 s
Ep. 96/200:	Train: ABL 5.354Val: ABL 5.481Time: 1058.8 s
Ep. 97/200:	Train: ABL 5.354Val: ABL 5.488Time: 1014.9 s
Ep. 98/200:	Train: ABL 5.354Val: ABL 5.508Time: 998.2 s
#### New Model Saved #####
Ep. 99/200:	Train: ABL 5.354Val: ABL 5.526Time: 951.6 s
Ep. 100/200:	Train: ABL 5.354Val: ABL 5.479Time: 967.2 s
#### New Model Saved #####
Ep. 101/200:	Train: ABL 5.354Val: ABL 5.483Time: 998.4 s
#### New Model Saved #####
Ep. 102/200:	Train: ABL 5.354Val: ABL 5.533Time: 1012.8 s
#### New Model Saved #####
Ep. 103/200:	Train: ABL 5.354Val: ABL 5.472Time: 1038.3 s
Ep. 104/200:	Train: ABL 5.354Val: ABL 5.503Time: 1057.3 s
#### New Model Saved #####
Ep. 105/200:	Train: ABL 5.354Val: ABL 5.505Time: 1047.0 s
Ep. 106/200:	Train: ABL 5.354Val: ABL 5.518Time: 1019.6 s
Ep. 107/200:	Train: ABL 5.354Val: ABL 5.487Time: 1074.1 s
#### New Model Saved #####
Ep. 108/200:	Train: ABL 5.354Val: ABL 5.453Time: 1051.3 s
#### New Model Saved #####
Ep. 109/200:	Train: ABL 5.354Val: ABL 5.487Time: 1025.2 s
#### New Model Saved #####
Ep. 110/200:	Train: ABL 5.354Val: ABL 5.466Time: 1030.1 s
#### New Model Saved #####
Ep. 111/200:	Train: ABL 5.354Val: ABL 5.464Time: 1026.0 s
Ep. 112/200:	Train: ABL 5.354Val: ABL 5.499Time: 1021.8 s
Ep. 113/200:	Train: ABL 5.354Val: ABL 5.482Time: 1019.9 s
Ep. 114/200:	Train: ABL 5.354Val: ABL 5.478Time: 1012.7 s
Ep. 115/200:	Train: ABL 5.354Val: ABL 5.48Time: 1019.7 s
Ep. 116/200:	Train: ABL 5.354Val: ABL 5.442Time: 973.0 s
Ep. 117/200:	Train: ABL 5.354Val: ABL 5.449Time: 980.4 s
Ep. 118/200:	Train: ABL 5.354Val: ABL 5.467Time: 996.0 s
Ep. 119/200:	Train: ABL 5.354Val: ABL 5.477Time: 978.5 s
Ep. 120/200:	Train: ABL 5.354Val: ABL 5.492Time: 979.9 s
#### New Model Saved #####
Ep. 120/200:	Epoch   111: adjusting learning rate of group 0 to 8.0076e-01.
Train: ABL 5.35377	Val: ABL 5.492,	Time: 979.9 s

#### New Model Saved #####
Ep. 121/200:	Epoch   112: adjusting learning rate of group 0 to 7.9904e-01.
Train: ABL 5.35379	Val: ABL 5.449,	Time: 1005.6 s

Ep. 122/200:	Epoch   113: adjusting learning rate of group 0 to 7.9730e-01.
Train: ABL 5.35379	Val: ABL 5.498,	Time: 998.0 s

Ep. 123/200:	Epoch   114: adjusting learning rate of group 0 to 7.9556e-01.
Train: ABL 5.35381	Val: ABL 5.502,	Time: 1027.6 s

Ep. 124/200:	Epoch   115: adjusting learning rate of group 0 to 7.9380e-01.
Train: ABL 5.35378	Val: ABL 5.463,	Time: 1019.4 s

Ep. 125/200:	Epoch   116: adjusting learning rate of group 0 to 7.9202e-01.
Train: ABL 5.35376	Val: ABL 5.446,	Time: 1017.8 s

#### New Model Saved #####
Ep. 126/200:	Epoch   117: adjusting learning rate of group 0 to 7.9024e-01.
Train: ABL 5.35376	Val: ABL 5.476,	Time: 989.7 s

#### New Model Saved #####
Ep. 127/200:	Epoch   118: adjusting learning rate of group 0 to 7.8844e-01.
Train: ABL 5.35377	Val: ABL 5.464,	Time: 984.0 s

Ep. 128/200:	Epoch   119: adjusting learning rate of group 0 to 7.8663e-01.
Train: ABL 5.35378	Val: ABL 5.475,	Time: 1000.6 s

Ep. 129/200:	Epoch   120: adjusting learning rate of group 0 to 7.8481e-01.
Train: ABL 5.35376	Val: ABL 5.454,	Time: 1021.1 s

#### New Model Saved #####
Ep. 130/200:	Epoch   121: adjusting learning rate of group 0 to 7.8298e-01.
Train: ABL 5.35374	Val: ABL 5.455,	Time: 1042.5 s

#### New Model Saved #####
Ep. 131/200:	Epoch   122: adjusting learning rate of group 0 to 7.8113e-01.
Train: ABL 5.35375	Val: ABL 5.453,	Time: 1048.5 s

Ep. 132/200:	Epoch   123: adjusting learning rate of group 0 to 7.7927e-01.
Train: ABL 5.35376	Val: ABL 5.476,	Time: 1027.1 s

Ep. 133/200:	Epoch   124: adjusting learning rate of group 0 to 7.7740e-01.
Train: ABL 5.35375	Val: ABL 5.438,	Time: 1032.8 s

Ep. 134/200:	Epoch   125: adjusting learning rate of group 0 to 7.7552e-01.
Train: ABL 5.35375	Val: ABL 5.474,	Time: 1045.5 s

Ep. 135/200:	Epoch   126: adjusting learning rate of group 0 to 7.7363e-01.
Train: ABL 5.35375	Val: ABL 5.485,	Time: 1053.7 s

Ep. 136/200:	Epoch   127: adjusting learning rate of group 0 to 7.7172e-01.
Train: ABL 5.35375	Val: ABL 5.483,	Time: 1037.5 s

Ep. 137/200:	Epoch   128: adjusting learning rate of group 0 to 7.6980e-01.
Train: ABL 5.35375	Val: ABL 5.457,	Time: 1007.9 s

Ep. 138/200:	Epoch   129: adjusting learning rate of group 0 to 7.6787e-01.
Train: ABL 5.35372	Val: ABL 5.47,	Time: 1004.8 s

#### New Model Saved #####
Ep. 139/200:	Epoch   130: adjusting learning rate of group 0 to 7.6593e-01.
Train: ABL 5.35375	Val: ABL 5.495,	Time: 1017.9 s

Ep. 140/200:	Epoch   131: adjusting learning rate of group 0 to 7.6398e-01.
Train: ABL 5.35373	Val: ABL 5.502,	Time: 973.3 s

#### New Model Saved #####
Ep. 141/200:	Epoch   132: adjusting learning rate of group 0 to 7.6202e-01.
Train: ABL 5.35374	Val: ABL 5.441,	Time: 1025.7 s

Ep. 142/200:	Epoch   133: adjusting learning rate of group 0 to 7.6004e-01.
Train: ABL 5.35375	Val: ABL 5.456,	Time: 994.2 s

Ep. 143/200:	Epoch   134: adjusting learning rate of group 0 to 7.5806e-01.
Train: ABL 5.35374	Val: ABL 5.482,	Time: 1040.6 s

Ep. 144/200:	Epoch   135: adjusting learning rate of group 0 to 7.5606e-01.
Train: ABL 5.35373	Val: ABL 5.49,	Time: 1029.0 s

Ep. 145/200:	Epoch   136: adjusting learning rate of group 0 to 7.5405e-01.
Train: ABL 5.35373	Val: ABL 5.474,	Time: 1020.7 s

Ep. 146/200:	Epoch   137: adjusting learning rate of group 0 to 7.5203e-01.
Train: ABL 5.35373	Val: ABL 5.448,	Time: 1013.4 s

Ep. 147/200:	Epoch   138: adjusting learning rate of group 0 to 7.5000e-01.
Train: ABL 5.35372	Val: ABL 5.478,	Time: 1011.6 s

#### New Model Saved #####
Ep. 148/200:	Epoch   139: adjusting learning rate of group 0 to 7.4796e-01.
Train: ABL 5.35373	Val: ABL 5.468,	Time: 996.1 s

Ep. 149/200:	Epoch   140: adjusting learning rate of group 0 to 7.4591e-01.
Train: ABL 5.35374	Val: ABL 5.456,	Time: 1066.2 s

Ep. 150/200:	Epoch   141: adjusting learning rate of group 0 to 7.4384e-01.
Train: ABL 5.35372	Val: ABL 5.484,	Time: 1025.1 s

#### New Model Saved #####
Ep. 151/200:	Epoch   142: adjusting learning rate of group 0 to 7.4177e-01.
Train: ABL 5.35373	Val: ABL 5.446,	Time: 1027.1 s

Ep. 152/200:	Epoch   143: adjusting learning rate of group 0 to 7.3968e-01.
Train: ABL 5.35374	Val: ABL 5.469,	Time: 1022.3 s

Ep. 153/200:	Epoch   144: adjusting learning rate of group 0 to 7.3759e-01.
Train: ABL 5.35374	Val: ABL 5.487,	Time: 1016.7 s

Ep. 154/200:	Epoch   145: adjusting learning rate of group 0 to 7.3549e-01.
Train: ABL 5.35376	Val: ABL 5.453,	Time: 1056.7 s

Ep. 155/200:	Epoch   146: adjusting learning rate of group 0 to 7.3337e-01.
Train: ABL 5.35373	Val: ABL 5.474,	Time: 1043.2 s

Ep. 156/200:	Epoch   147: adjusting learning rate of group 0 to 7.3125e-01.
Train: ABL 5.35371	Val: ABL 5.433,	Time: 1044.4 s

#### New Model Saved #####
Ep. 157/200:	Epoch   148: adjusting learning rate of group 0 to 7.2911e-01.
Train: ABL 5.35372	Val: ABL 5.429,	Time: 995.9 s

Ep. 158/200:	Epoch   149: adjusting learning rate of group 0 to 7.2696e-01.
Train: ABL 5.35371	Val: ABL 5.437,	Time: 1003.4 s

#### New Model Saved #####
Ep. 159/200:	Epoch   150: adjusting learning rate of group 0 to 7.2481e-01.
Train: ABL 5.35371	Val: ABL 5.471,	Time: 1035.5 s

Ep. 160/200:	Epoch   151: adjusting learning rate of group 0 to 7.2264e-01.
Train: ABL 5.35371	Val: ABL 5.454,	Time: 1036.0 s

#### New Model Saved #####
Ep. 161/200:	Epoch   152: adjusting learning rate of group 0 to 7.2047e-01.
Train: ABL 5.3537	Val: ABL 5.461,	Time: 1011.9 s

#### New Model Saved #####
Ep. 162/200:	Epoch   153: adjusting learning rate of group 0 to 7.1828e-01.
Train: ABL 5.35371	Val: ABL 5.425,	Time: 1014.3 s

Ep. 163/200:	Epoch   154: adjusting learning rate of group 0 to 7.1609e-01.
Train: ABL 5.35372	Val: ABL 5.444,	Time: 977.5 s

Ep. 164/200:	Epoch   155: adjusting learning rate of group 0 to 7.1389e-01.
Train: ABL 5.35372	Val: ABL 5.452,	Time: 1031.1 s

Ep. 165/200:	Epoch   156: adjusting learning rate of group 0 to 7.1167e-01.
Train: ABL 5.35372	Val: ABL 5.458,	Time: 1018.6 s

Ep. 166/200:	Epoch   157: adjusting learning rate of group 0 to 7.0945e-01.
Train: ABL 5.35371	Val: ABL 5.442,	Time: 1015.0 s

Ep. 167/200:	Epoch   158: adjusting learning rate of group 0 to 7.0722e-01.
Train: ABL 5.35371	Val: ABL 5.464,	Time: 1029.7 s

Ep. 168/200:	Epoch   159: adjusting learning rate of group 0 to 7.0498e-01.
Train: ABL 5.35371	Val: ABL 5.466,	Time: 1046.2 s

Ep. 169/200:	Epoch   160: adjusting learning rate of group 0 to 7.0273e-01.
Train: ABL 5.35372	Val: ABL 5.463,	Time: 1044.4 s

Ep. 170/200:	Epoch   161: adjusting learning rate of group 0 to 7.0047e-01.
Train: ABL 5.35369	Val: ABL 5.447,	Time: 1007.2 s

#### New Model Saved #####
Ep. 171/200:	Epoch   162: adjusting learning rate of group 0 to 6.9820e-01.
Train: ABL 5.35371	Val: ABL 5.445,	Time: 977.2 s

Ep. 172/200:	Epoch   163: adjusting learning rate of group 0 to 6.9592e-01.
Train: ABL 5.3537	Val: ABL 5.455,	Time: 1014.3 s

Ep. 173/200:	Epoch   164: adjusting learning rate of group 0 to 6.9364e-01.
Train: ABL 5.3537	Val: ABL 5.458,	Time: 999.9 s

Ep. 174/200:	Epoch   165: adjusting learning rate of group 0 to 6.9134e-01.
Train: ABL 5.3537	Val: ABL 5.431,	Time: 983.6 s

Ep. 175/200:	Epoch   166: adjusting learning rate of group 0 to 6.8904e-01.
Train: ABL 5.3537	Val: ABL 5.422,	Time: 980.9 s

Ep. 176/200:	Epoch   167: adjusting learning rate of group 0 to 6.8673e-01.
Train: ABL 5.35368	Val: ABL 5.417,	Time: 981.9 s

#### New Model Saved #####
Ep. 177/200:	Epoch   168: adjusting learning rate of group 0 to 6.8441e-01.
Train: ABL 5.3537	Val: ABL 5.424,	Time: 980.4 s

Ep. 178/200:	Epoch   169: adjusting learning rate of group 0 to 6.8208e-01.
Train: ABL 5.35368	Val: ABL 5.426,	Time: 988.9 s

#### New Model Saved #####
Ep. 179/200:	Epoch   170: adjusting learning rate of group 0 to 6.7975e-01.
Train: ABL 5.35367	Val: ABL 5.42,	Time: 1032.0 s

#### New Model Saved #####
Ep. 180/200:	Epoch   171: adjusting learning rate of group 0 to 6.7740e-01.
Train: ABL 5.35369	Val: ABL 5.419,	Time: 1021.8 s

#### New Model Saved #####
Ep. 181/200:	Epoch   172: adjusting learning rate of group 0 to 6.7505e-01.
Train: ABL 5.35369	Val: ABL 5.436,	Time: 1025.2 s

Ep. 182/200:	Epoch   173: adjusting learning rate of group 0 to 6.7269e-01.
Train: ABL 5.3537	Val: ABL 5.438,	Time: 1029.5 s

Ep. 183/200:	Epoch   174: adjusting learning rate of group 0 to 6.7032e-01.
Train: ABL 5.3537	Val: ABL 5.442,	Time: 1043.0 s

Ep. 184/200:	Epoch   175: adjusting learning rate of group 0 to 6.6795e-01.
Train: ABL 5.3537	Val: ABL 5.436,	Time: 987.9 s

Ep. 185/200:	Epoch   176: adjusting learning rate of group 0 to 6.6556e-01.
Train: ABL 5.3537	Val: ABL 5.436,	Time: 1068.6 s

Ep. 186/200:	Epoch   177: adjusting learning rate of group 0 to 6.6317e-01.
Train: ABL 5.35368	Val: ABL 5.443,	Time: 1048.9 s

Ep. 187/200:	Epoch   178: adjusting learning rate of group 0 to 6.6077e-01.
Train: ABL 5.35369	Val: ABL 5.445,	Time: 1029.5 s

Ep. 188/200:	Epoch   179: adjusting learning rate of group 0 to 6.5837e-01.
Train: ABL 5.3537	Val: ABL 5.434,	Time: 1042.5 s

Ep. 189/200:	Epoch   180: adjusting learning rate of group 0 to 6.5596e-01.
Train: ABL 5.35369	Val: ABL 5.448,	Time: 1035.0 s

Ep. 190/200:	Epoch   181: adjusting learning rate of group 0 to 6.5354e-01.
Train: ABL 5.35368	Val: ABL 5.446,	Time: 1000.6 s

#### New Model Saved #####
Ep. 191/200:	Epoch   182: adjusting learning rate of group 0 to 6.5111e-01.
Train: ABL 5.35369	Val: ABL 5.451,	Time: 1034.4 s

Ep. 192/200:	Epoch   183: adjusting learning rate of group 0 to 6.4868e-01.
Train: ABL 5.35368	Val: ABL 5.451,	Time: 1012.6 s

Ep. 193/200:	Epoch   184: adjusting learning rate of group 0 to 6.4624e-01.
Train: ABL 5.35368	Val: ABL 5.425,	Time: 976.9 s

Ep. 194/200:	Epoch   185: adjusting learning rate of group 0 to 6.4379e-01.
Train: ABL 5.35369	Val: ABL 5.44,	Time: 1022.3 s

Ep. 195/200:	Epoch   186: adjusting learning rate of group 0 to 6.4133e-01.
Train: ABL 5.35369	Val: ABL 5.427,	Time: 1030.2 s

Ep. 196/200:	Epoch   187: adjusting learning rate of group 0 to 6.3887e-01.
Train: ABL 5.35369	Val: ABL 5.463,	Time: 1034.8 s

Ep. 197/200:	Epoch   188: adjusting learning rate of group 0 to 6.3641e-01.
Train: ABL 5.35368	Val: ABL 5.425,	Time: 1040.3 s

Ep. 198/200:	Epoch   189: adjusting learning rate of group 0 to 6.3393e-01.
Train: ABL 5.35368	Val: ABL 5.438,	Time: 1025.4 s

Ep. 199/200:	Epoch   190: adjusting learning rate of group 0 to 6.3145e-01.
Train: ABL 5.35368	Val: ABL 5.422,	Time: 1010.1 s

#### Ended Training ####
---------------------------------------------------------------------------
